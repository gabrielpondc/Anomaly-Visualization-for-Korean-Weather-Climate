{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import sys\n",
    "from embedding import Graph2Vec\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection_str = 'mysql+pymysql://root:caucse1234@avwc.caucse.club/virus'\n",
    "db_connection = create_engine(db_connection_str)\n",
    "df = pd.read_sql('SELECT test.city,dataset.score FROM dataset,test where dataset.KID=test.KID and test.city=\"부산광역시\" and dataset.date =\"2018-01-01\"', con=db_connection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>부산광역시</td>\n",
       "      <td>1.36861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    city    score\n",
       "0  부산광역시  1.36861"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processingforcorr(data):\n",
    "        Corr = data.corr().values\n",
    "        Corr = np.abs(Corr)\n",
    "        Corr = pd.DataFrame(Corr)\n",
    "        return Corr.fillna(0).values\n",
    "\n",
    "def Graph(data):\n",
    "    maxlag = 1\n",
    "    test = 'ssr_chi2test'\n",
    "    X = []\n",
    "    new = processingforcorr(data.iloc[:24])\n",
    "    X.append(new)\n",
    "    i = 1\n",
    "    while i<=data.shape[0]-25:\n",
    "        new = processingforcorr(data.iloc[i:i+24])\n",
    "        X.append(new)\n",
    "        i=i+24\n",
    "            \n",
    "    return np.array(X)\n",
    "\n",
    "def entropy(X):\n",
    "        E = []\n",
    "        for i in range(X.shape[0]):\n",
    "            P = []\n",
    "            for j in range(X.shape[1]):\n",
    "                if i !=j:\n",
    "                    e = -X[i][j]*np.log(X[i][j])\n",
    "                    P.append(e)\n",
    "            P = np.array(P)\n",
    "            E.append(np.sum(P))\n",
    "        return np.array(E)\n",
    "\n",
    "def graphentropy(X):\n",
    "        E = []\n",
    "        for i in range(X.shape[0]):\n",
    "            e = entropy(X[i])\n",
    "            E.append(np.sum(e))\n",
    "        return np.array(E)\n",
    "\n",
    "def distance(x,y):\n",
    "        distance = np.mean(np.power((x - y),2))\n",
    "        return distance\n",
    "\n",
    "def getMatrix(data,E):\n",
    "        Matrix = []\n",
    "        for i in range(E.shape[0]):\n",
    "            dis = []\n",
    "            for j in range(len(E)):\n",
    "                dis.append(distance(E[i],E[j]))\n",
    "            dis = np.array(dis)\n",
    "            index = np.argsort(dis)[1]\n",
    "            Matrix.append(data[index])\n",
    "            \n",
    "        return np.array(Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Graph(df1)\n",
    "X = x.reshape(-1,49)\n",
    "pca = PCA(n_components=4*4)\n",
    "X= pca.fit_transform(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        if X[i][j] == 0:\n",
    "                X[i][j] = 1\n",
    "X = np.abs(X.reshape(X.shape[0],4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = graphentropy(X)\n",
    "\n",
    "MatrixX = getMatrix(X,E)\n",
    "\n",
    "TrainXTensor = torch.from_numpy(X.reshape(X.shape[0], 16)).type(torch.FloatTensor)\n",
    "TrainSTensor = torch.from_numpy(MatrixX.reshape(X.shape[0], 16)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1095, 16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainSTensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph2Vec(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(Graph2Vec, self).__init__()\n",
    "\n",
    "        self.encode1 = nn.Sequential(nn.Linear(n_input, 10),\n",
    "                                     nn.Dropout(0.7),\n",
    "                                     nn.Tanh(),\n",
    "\n",
    "                                     nn.Linear(10, 9),\n",
    "                                     nn.Dropout(0.7),\n",
    "                                     nn.Tanh(),\n",
    "\n",
    "                                     nn.Linear(9, 9),\n",
    "                                     nn.Dropout(0.7),\n",
    "                                     nn.Tanh(),\n",
    "\n",
    "                                     nn.Linear(9, 10),\n",
    "                                     nn.Dropout(0.7),\n",
    "                                     nn.Tanh(),\n",
    "                                     )\n",
    "        self.decode1 = nn.Sequential(nn.Linear(10, 9),\n",
    "                                     nn.Dropout(0.7),\n",
    "                                     nn.Tanh(),\n",
    "\n",
    "                                     nn.Linear(9, 9),\n",
    "                                     nn.Dropout(0.7),\n",
    "                                     nn.Tanh(),\n",
    "\n",
    "                                     nn.Linear(9, 10),\n",
    "                                     nn.Dropout(0.7),\n",
    "                                     nn.Tanh(),\n",
    "\n",
    "                                     nn.Linear(10, n_output), )\n",
    "\n",
    "    def forward(self, x, matrix):\n",
    "        ed1 = self.encode1(x)\n",
    "        ed2 = self.encode1(matrix)\n",
    "\n",
    "        de1 = self.decode1(ed1)\n",
    "        de2 = self.decode1(ed2)\n",
    "\n",
    "        return ed1, ed2, de1, de2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x,de1,s,de2,ed1,ed2):\n",
    "        l1=torch.mean(torch.pow((x-de1),2))\n",
    "        l2=torch.mean(torch.pow((s-de2),2))\n",
    "        l3=torch.mean(torch.pow((ed1-ed2),2))\n",
    "        loss=l1+l2+l3\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph2vec=Graph2Vec(16,16)\n",
    "optimizer = torch.optim.Adam(graph2vec.parameters(), lr=0.001)\n",
    "loss_func = My_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Traininng loss is 0.6764\n",
      "Epoch:  1 | Traininng loss is 0.6753\n",
      "Epoch:  2 | Traininng loss is 0.6748\n",
      "Epoch:  3 | Traininng loss is 0.6606\n",
      "Epoch:  4 | Traininng loss is 0.6620\n",
      "Epoch:  5 | Traininng loss is 0.6498\n",
      "Epoch:  6 | Traininng loss is 0.6404\n",
      "Epoch:  7 | Traininng loss is 0.6373\n",
      "Epoch:  8 | Traininng loss is 0.6334\n",
      "Epoch:  9 | Traininng loss is 0.6208\n",
      "Epoch:  10 | Traininng loss is 0.6210\n",
      "Epoch:  11 | Traininng loss is 0.6242\n",
      "Epoch:  12 | Traininng loss is 0.6122\n",
      "Epoch:  13 | Traininng loss is 0.6080\n",
      "Epoch:  14 | Traininng loss is 0.6097\n",
      "Epoch:  15 | Traininng loss is 0.5968\n",
      "Epoch:  16 | Traininng loss is 0.5918\n",
      "Epoch:  17 | Traininng loss is 0.5842\n",
      "Epoch:  18 | Traininng loss is 0.5790\n",
      "Epoch:  19 | Traininng loss is 0.5694\n",
      "Epoch:  20 | Traininng loss is 0.5672\n",
      "Epoch:  21 | Traininng loss is 0.5605\n",
      "Epoch:  22 | Traininng loss is 0.5589\n",
      "Epoch:  23 | Traininng loss is 0.5491\n",
      "Epoch:  24 | Traininng loss is 0.5489\n",
      "Epoch:  25 | Traininng loss is 0.5426\n",
      "Epoch:  26 | Traininng loss is 0.5397\n",
      "Epoch:  27 | Traininng loss is 0.5352\n",
      "Epoch:  28 | Traininng loss is 0.5318\n",
      "Epoch:  29 | Traininng loss is 0.5237\n",
      "Epoch:  30 | Traininng loss is 0.5229\n",
      "Epoch:  31 | Traininng loss is 0.5175\n",
      "Epoch:  32 | Traininng loss is 0.5157\n",
      "Epoch:  33 | Traininng loss is 0.5091\n",
      "Epoch:  34 | Traininng loss is 0.5028\n",
      "Epoch:  35 | Traininng loss is 0.4999\n",
      "Epoch:  36 | Traininng loss is 0.4911\n",
      "Epoch:  37 | Traininng loss is 0.4892\n",
      "Epoch:  38 | Traininng loss is 0.4890\n",
      "Epoch:  39 | Traininng loss is 0.4802\n",
      "Epoch:  40 | Traininng loss is 0.4783\n",
      "Epoch:  41 | Traininng loss is 0.4772\n",
      "Epoch:  42 | Traininng loss is 0.4683\n",
      "Epoch:  43 | Traininng loss is 0.4653\n",
      "Epoch:  44 | Traininng loss is 0.4609\n",
      "Epoch:  45 | Traininng loss is 0.4617\n",
      "Epoch:  46 | Traininng loss is 0.4586\n",
      "Epoch:  47 | Traininng loss is 0.4518\n",
      "Epoch:  48 | Traininng loss is 0.4476\n",
      "Epoch:  49 | Traininng loss is 0.4467\n",
      "Epoch:  50 | Traininng loss is 0.4411\n",
      "Epoch:  51 | Traininng loss is 0.4383\n",
      "Epoch:  52 | Traininng loss is 0.4331\n",
      "Epoch:  53 | Traininng loss is 0.4327\n",
      "Epoch:  54 | Traininng loss is 0.4284\n",
      "Epoch:  55 | Traininng loss is 0.4268\n",
      "Epoch:  56 | Traininng loss is 0.4190\n",
      "Epoch:  57 | Traininng loss is 0.4148\n",
      "Epoch:  58 | Traininng loss is 0.4136\n",
      "Epoch:  59 | Traininng loss is 0.4151\n",
      "Epoch:  60 | Traininng loss is 0.4087\n",
      "Epoch:  61 | Traininng loss is 0.4037\n",
      "Epoch:  62 | Traininng loss is 0.3994\n",
      "Epoch:  63 | Traininng loss is 0.3978\n",
      "Epoch:  64 | Traininng loss is 0.3950\n",
      "Epoch:  65 | Traininng loss is 0.3914\n",
      "Epoch:  66 | Traininng loss is 0.3872\n",
      "Epoch:  67 | Traininng loss is 0.3862\n",
      "Epoch:  68 | Traininng loss is 0.3811\n",
      "Epoch:  69 | Traininng loss is 0.3785\n",
      "Epoch:  70 | Traininng loss is 0.3768\n",
      "Epoch:  71 | Traininng loss is 0.3739\n",
      "Epoch:  72 | Traininng loss is 0.3729\n",
      "Epoch:  73 | Traininng loss is 0.3647\n",
      "Epoch:  74 | Traininng loss is 0.3632\n",
      "Epoch:  75 | Traininng loss is 0.3632\n",
      "Epoch:  76 | Traininng loss is 0.3633\n",
      "Epoch:  77 | Traininng loss is 0.3538\n",
      "Epoch:  78 | Traininng loss is 0.3547\n",
      "Epoch:  79 | Traininng loss is 0.3521\n",
      "Epoch:  80 | Traininng loss is 0.3474\n",
      "Epoch:  81 | Traininng loss is 0.3444\n",
      "Epoch:  82 | Traininng loss is 0.3418\n",
      "Epoch:  83 | Traininng loss is 0.3378\n",
      "Epoch:  84 | Traininng loss is 0.3389\n",
      "Epoch:  85 | Traininng loss is 0.3326\n",
      "Epoch:  86 | Traininng loss is 0.3314\n",
      "Epoch:  87 | Traininng loss is 0.3286\n",
      "Epoch:  88 | Traininng loss is 0.3269\n",
      "Epoch:  89 | Traininng loss is 0.3233\n",
      "Epoch:  90 | Traininng loss is 0.3246\n",
      "Epoch:  91 | Traininng loss is 0.3213\n",
      "Epoch:  92 | Traininng loss is 0.3181\n",
      "Epoch:  93 | Traininng loss is 0.3140\n",
      "Epoch:  94 | Traininng loss is 0.3123\n",
      "Epoch:  95 | Traininng loss is 0.3103\n",
      "Epoch:  96 | Traininng loss is 0.3091\n",
      "Epoch:  97 | Traininng loss is 0.3074\n",
      "Epoch:  98 | Traininng loss is 0.3049\n",
      "Epoch:  99 | Traininng loss is 0.3039\n",
      "Epoch:  100 | Traininng loss is 0.3016\n",
      "Epoch:  101 | Traininng loss is 0.2981\n",
      "Epoch:  102 | Traininng loss is 0.2980\n",
      "Epoch:  103 | Traininng loss is 0.2926\n",
      "Epoch:  104 | Traininng loss is 0.2909\n",
      "Epoch:  105 | Traininng loss is 0.2896\n",
      "Epoch:  106 | Traininng loss is 0.2875\n",
      "Epoch:  107 | Traininng loss is 0.2831\n",
      "Epoch:  108 | Traininng loss is 0.2794\n",
      "Epoch:  109 | Traininng loss is 0.2805\n",
      "Epoch:  110 | Traininng loss is 0.2764\n",
      "Epoch:  111 | Traininng loss is 0.2788\n",
      "Epoch:  112 | Traininng loss is 0.2730\n",
      "Epoch:  113 | Traininng loss is 0.2714\n",
      "Epoch:  114 | Traininng loss is 0.2728\n",
      "Epoch:  115 | Traininng loss is 0.2718\n",
      "Epoch:  116 | Traininng loss is 0.2660\n",
      "Epoch:  117 | Traininng loss is 0.2659\n",
      "Epoch:  118 | Traininng loss is 0.2634\n",
      "Epoch:  119 | Traininng loss is 0.2652\n",
      "Epoch:  120 | Traininng loss is 0.2601\n",
      "Epoch:  121 | Traininng loss is 0.2571\n",
      "Epoch:  122 | Traininng loss is 0.2589\n",
      "Epoch:  123 | Traininng loss is 0.2571\n",
      "Epoch:  124 | Traininng loss is 0.2544\n",
      "Epoch:  125 | Traininng loss is 0.2526\n",
      "Epoch:  126 | Traininng loss is 0.2510\n",
      "Epoch:  127 | Traininng loss is 0.2512\n",
      "Epoch:  128 | Traininng loss is 0.2501\n",
      "Epoch:  129 | Traininng loss is 0.2467\n",
      "Epoch:  130 | Traininng loss is 0.2491\n",
      "Epoch:  131 | Traininng loss is 0.2429\n",
      "Epoch:  132 | Traininng loss is 0.2422\n",
      "Epoch:  133 | Traininng loss is 0.2434\n",
      "Epoch:  134 | Traininng loss is 0.2382\n",
      "Epoch:  135 | Traininng loss is 0.2382\n",
      "Epoch:  136 | Traininng loss is 0.2360\n",
      "Epoch:  137 | Traininng loss is 0.2373\n",
      "Epoch:  138 | Traininng loss is 0.2351\n",
      "Epoch:  139 | Traininng loss is 0.2303\n",
      "Epoch:  140 | Traininng loss is 0.2304\n",
      "Epoch:  141 | Traininng loss is 0.2294\n",
      "Epoch:  142 | Traininng loss is 0.2275\n",
      "Epoch:  143 | Traininng loss is 0.2267\n",
      "Epoch:  144 | Traininng loss is 0.2266\n",
      "Epoch:  145 | Traininng loss is 0.2276\n",
      "Epoch:  146 | Traininng loss is 0.2225\n",
      "Epoch:  147 | Traininng loss is 0.2229\n",
      "Epoch:  148 | Traininng loss is 0.2211\n",
      "Epoch:  149 | Traininng loss is 0.2203\n",
      "Epoch:  150 | Traininng loss is 0.2203\n",
      "Epoch:  151 | Traininng loss is 0.2174\n",
      "Epoch:  152 | Traininng loss is 0.2177\n",
      "Epoch:  153 | Traininng loss is 0.2155\n",
      "Epoch:  154 | Traininng loss is 0.2163\n",
      "Epoch:  155 | Traininng loss is 0.2133\n",
      "Epoch:  156 | Traininng loss is 0.2121\n",
      "Epoch:  157 | Traininng loss is 0.2134\n",
      "Epoch:  158 | Traininng loss is 0.2111\n",
      "Epoch:  159 | Traininng loss is 0.2114\n",
      "Epoch:  160 | Traininng loss is 0.2080\n",
      "Epoch:  161 | Traininng loss is 0.2101\n",
      "Epoch:  162 | Traininng loss is 0.2077\n",
      "Epoch:  163 | Traininng loss is 0.2070\n",
      "Epoch:  164 | Traininng loss is 0.2053\n",
      "Epoch:  165 | Traininng loss is 0.2090\n",
      "Epoch:  166 | Traininng loss is 0.2038\n",
      "Epoch:  167 | Traininng loss is 0.2045\n",
      "Epoch:  168 | Traininng loss is 0.2042\n",
      "Epoch:  169 | Traininng loss is 0.2011\n",
      "Epoch:  170 | Traininng loss is 0.2007\n",
      "Epoch:  171 | Traininng loss is 0.1966\n",
      "Epoch:  172 | Traininng loss is 0.2014\n",
      "Epoch:  173 | Traininng loss is 0.1987\n",
      "Epoch:  174 | Traininng loss is 0.1979\n",
      "Epoch:  175 | Traininng loss is 0.1952\n",
      "Epoch:  176 | Traininng loss is 0.1972\n",
      "Epoch:  177 | Traininng loss is 0.1941\n",
      "Epoch:  178 | Traininng loss is 0.1936\n",
      "Epoch:  179 | Traininng loss is 0.1941\n",
      "Epoch:  180 | Traininng loss is 0.1921\n",
      "Epoch:  181 | Traininng loss is 0.1937\n",
      "Epoch:  182 | Traininng loss is 0.1907\n",
      "Epoch:  183 | Traininng loss is 0.1939\n",
      "Epoch:  184 | Traininng loss is 0.1920\n",
      "Epoch:  185 | Traininng loss is 0.1897\n",
      "Epoch:  186 | Traininng loss is 0.1890\n",
      "Epoch:  187 | Traininng loss is 0.1904\n",
      "Epoch:  188 | Traininng loss is 0.1887\n",
      "Epoch:  189 | Traininng loss is 0.1878\n",
      "Epoch:  190 | Traininng loss is 0.1856\n",
      "Epoch:  191 | Traininng loss is 0.1883\n",
      "Epoch:  192 | Traininng loss is 0.1865\n",
      "Epoch:  193 | Traininng loss is 0.1840\n",
      "Epoch:  194 | Traininng loss is 0.1832\n",
      "Epoch:  195 | Traininng loss is 0.1859\n",
      "Epoch:  196 | Traininng loss is 0.1842\n",
      "Epoch:  197 | Traininng loss is 0.1843\n",
      "Epoch:  198 | Traininng loss is 0.1836\n",
      "Epoch:  199 | Traininng loss is 0.1839\n",
      "Epoch:  200 | Traininng loss is 0.1830\n",
      "Epoch:  201 | Traininng loss is 0.1820\n",
      "Epoch:  202 | Traininng loss is 0.1798\n",
      "Epoch:  203 | Traininng loss is 0.1798\n",
      "Epoch:  204 | Traininng loss is 0.1827\n",
      "Epoch:  205 | Traininng loss is 0.1783\n",
      "Epoch:  206 | Traininng loss is 0.1772\n",
      "Epoch:  207 | Traininng loss is 0.1746\n",
      "Epoch:  208 | Traininng loss is 0.1772\n",
      "Epoch:  209 | Traininng loss is 0.1743\n",
      "Epoch:  210 | Traininng loss is 0.1790\n",
      "Epoch:  211 | Traininng loss is 0.1758\n",
      "Epoch:  212 | Traininng loss is 0.1772\n",
      "Epoch:  213 | Traininng loss is 0.1739\n",
      "Epoch:  214 | Traininng loss is 0.1771\n",
      "Epoch:  215 | Traininng loss is 0.1738\n",
      "Epoch:  216 | Traininng loss is 0.1731\n",
      "Epoch:  217 | Traininng loss is 0.1742\n",
      "Epoch:  218 | Traininng loss is 0.1720\n",
      "Epoch:  219 | Traininng loss is 0.1720\n",
      "Epoch:  220 | Traininng loss is 0.1739\n",
      "Epoch:  221 | Traininng loss is 0.1718\n",
      "Epoch:  222 | Traininng loss is 0.1733\n",
      "Epoch:  223 | Traininng loss is 0.1707\n",
      "Epoch:  224 | Traininng loss is 0.1704\n",
      "Epoch:  225 | Traininng loss is 0.1697\n",
      "Epoch:  226 | Traininng loss is 0.1704\n",
      "Epoch:  227 | Traininng loss is 0.1701\n",
      "Epoch:  228 | Traininng loss is 0.1699\n",
      "Epoch:  229 | Traininng loss is 0.1687\n",
      "Epoch:  230 | Traininng loss is 0.1697\n",
      "Epoch:  231 | Traininng loss is 0.1703\n",
      "Epoch:  232 | Traininng loss is 0.1688\n",
      "Epoch:  233 | Traininng loss is 0.1665\n",
      "Epoch:  234 | Traininng loss is 0.1673\n",
      "Epoch:  235 | Traininng loss is 0.1655\n",
      "Epoch:  236 | Traininng loss is 0.1650\n",
      "Epoch:  237 | Traininng loss is 0.1657\n",
      "Epoch:  238 | Traininng loss is 0.1648\n",
      "Epoch:  239 | Traininng loss is 0.1662\n",
      "Epoch:  240 | Traininng loss is 0.1647\n",
      "Epoch:  241 | Traininng loss is 0.1663\n",
      "Epoch:  242 | Traininng loss is 0.1658\n",
      "Epoch:  243 | Traininng loss is 0.1643\n",
      "Epoch:  244 | Traininng loss is 0.1647\n",
      "Epoch:  245 | Traininng loss is 0.1644\n",
      "Epoch:  246 | Traininng loss is 0.1644\n",
      "Epoch:  247 | Traininng loss is 0.1611\n",
      "Epoch:  248 | Traininng loss is 0.1632\n",
      "Epoch:  249 | Traininng loss is 0.1632\n",
      "Epoch:  250 | Traininng loss is 0.1612\n",
      "Epoch:  251 | Traininng loss is 0.1621\n",
      "Epoch:  252 | Traininng loss is 0.1607\n",
      "Epoch:  253 | Traininng loss is 0.1614\n",
      "Epoch:  254 | Traininng loss is 0.1618\n",
      "Epoch:  255 | Traininng loss is 0.1602\n",
      "Epoch:  256 | Traininng loss is 0.1635\n",
      "Epoch:  257 | Traininng loss is 0.1602\n",
      "Epoch:  258 | Traininng loss is 0.1623\n",
      "Epoch:  259 | Traininng loss is 0.1597\n",
      "Epoch:  260 | Traininng loss is 0.1606\n",
      "Epoch:  261 | Traininng loss is 0.1596\n",
      "Epoch:  262 | Traininng loss is 0.1605\n",
      "Epoch:  263 | Traininng loss is 0.1597\n",
      "Epoch:  264 | Traininng loss is 0.1600\n",
      "Epoch:  265 | Traininng loss is 0.1607\n",
      "Epoch:  266 | Traininng loss is 0.1570\n",
      "Epoch:  267 | Traininng loss is 0.1593\n",
      "Epoch:  268 | Traininng loss is 0.1575\n",
      "Epoch:  269 | Traininng loss is 0.1588\n",
      "Epoch:  270 | Traininng loss is 0.1557\n",
      "Epoch:  271 | Traininng loss is 0.1581\n",
      "Epoch:  272 | Traininng loss is 0.1564\n",
      "Epoch:  273 | Traininng loss is 0.1580\n",
      "Epoch:  274 | Traininng loss is 0.1569\n",
      "Epoch:  275 | Traininng loss is 0.1559\n",
      "Epoch:  276 | Traininng loss is 0.1553\n",
      "Epoch:  277 | Traininng loss is 0.1555\n",
      "Epoch:  278 | Traininng loss is 0.1550\n",
      "Epoch:  279 | Traininng loss is 0.1548\n",
      "Epoch:  280 | Traininng loss is 0.1546\n",
      "Epoch:  281 | Traininng loss is 0.1578\n",
      "Epoch:  282 | Traininng loss is 0.1574\n",
      "Epoch:  283 | Traininng loss is 0.1560\n",
      "Epoch:  284 | Traininng loss is 0.1533\n",
      "Epoch:  285 | Traininng loss is 0.1559\n",
      "Epoch:  286 | Traininng loss is 0.1540\n",
      "Epoch:  287 | Traininng loss is 0.1542\n",
      "Epoch:  288 | Traininng loss is 0.1574\n",
      "Epoch:  289 | Traininng loss is 0.1540\n",
      "Epoch:  290 | Traininng loss is 0.1539\n",
      "Epoch:  291 | Traininng loss is 0.1541\n",
      "Epoch:  292 | Traininng loss is 0.1554\n",
      "Epoch:  293 | Traininng loss is 0.1531\n",
      "Epoch:  294 | Traininng loss is 0.1522\n",
      "Epoch:  295 | Traininng loss is 0.1526\n",
      "Epoch:  296 | Traininng loss is 0.1528\n",
      "Epoch:  297 | Traininng loss is 0.1521\n",
      "Epoch:  298 | Traininng loss is 0.1507\n",
      "Epoch:  299 | Traininng loss is 0.1525\n",
      "Epoch:  300 | Traininng loss is 0.1511\n",
      "Epoch:  301 | Traininng loss is 0.1502\n",
      "Epoch:  302 | Traininng loss is 0.1505\n",
      "Epoch:  303 | Traininng loss is 0.1518\n",
      "Epoch:  304 | Traininng loss is 0.1497\n",
      "Epoch:  305 | Traininng loss is 0.1519\n",
      "Epoch:  306 | Traininng loss is 0.1517\n",
      "Epoch:  307 | Traininng loss is 0.1507\n",
      "Epoch:  308 | Traininng loss is 0.1498\n",
      "Epoch:  309 | Traininng loss is 0.1503\n",
      "Epoch:  310 | Traininng loss is 0.1518\n",
      "Epoch:  311 | Traininng loss is 0.1460\n",
      "Epoch:  312 | Traininng loss is 0.1510\n",
      "Epoch:  313 | Traininng loss is 0.1488\n",
      "Epoch:  314 | Traininng loss is 0.1510\n",
      "Epoch:  315 | Traininng loss is 0.1498\n",
      "Epoch:  316 | Traininng loss is 0.1511\n",
      "Epoch:  317 | Traininng loss is 0.1491\n",
      "Epoch:  318 | Traininng loss is 0.1492\n",
      "Epoch:  319 | Traininng loss is 0.1503\n",
      "Epoch:  320 | Traininng loss is 0.1492\n",
      "Epoch:  321 | Traininng loss is 0.1493\n",
      "Epoch:  322 | Traininng loss is 0.1489\n",
      "Epoch:  323 | Traininng loss is 0.1495\n",
      "Epoch:  324 | Traininng loss is 0.1486\n",
      "Epoch:  325 | Traininng loss is 0.1499\n",
      "Epoch:  326 | Traininng loss is 0.1515\n",
      "Epoch:  327 | Traininng loss is 0.1486\n",
      "Epoch:  328 | Traininng loss is 0.1478\n",
      "Epoch:  329 | Traininng loss is 0.1496\n",
      "Epoch:  330 | Traininng loss is 0.1470\n",
      "Epoch:  331 | Traininng loss is 0.1501\n",
      "Epoch:  332 | Traininng loss is 0.1466\n",
      "Epoch:  333 | Traininng loss is 0.1490\n",
      "Epoch:  334 | Traininng loss is 0.1468\n",
      "Epoch:  335 | Traininng loss is 0.1476\n",
      "Epoch:  336 | Traininng loss is 0.1466\n",
      "Epoch:  337 | Traininng loss is 0.1451\n",
      "Epoch:  338 | Traininng loss is 0.1476\n",
      "Epoch:  339 | Traininng loss is 0.1453\n",
      "Epoch:  340 | Traininng loss is 0.1473\n",
      "Epoch:  341 | Traininng loss is 0.1461\n",
      "Epoch:  342 | Traininng loss is 0.1452\n",
      "Epoch:  343 | Traininng loss is 0.1463\n",
      "Epoch:  344 | Traininng loss is 0.1455\n",
      "Epoch:  345 | Traininng loss is 0.1444\n",
      "Epoch:  346 | Traininng loss is 0.1458\n",
      "Epoch:  347 | Traininng loss is 0.1451\n",
      "Epoch:  348 | Traininng loss is 0.1445\n",
      "Epoch:  349 | Traininng loss is 0.1466\n",
      "Epoch:  350 | Traininng loss is 0.1455\n",
      "Epoch:  351 | Traininng loss is 0.1452\n",
      "Epoch:  352 | Traininng loss is 0.1437\n",
      "Epoch:  353 | Traininng loss is 0.1456\n",
      "Epoch:  354 | Traininng loss is 0.1446\n",
      "Epoch:  355 | Traininng loss is 0.1431\n",
      "Epoch:  356 | Traininng loss is 0.1429\n",
      "Epoch:  357 | Traininng loss is 0.1438\n",
      "Epoch:  358 | Traininng loss is 0.1453\n",
      "Epoch:  359 | Traininng loss is 0.1424\n",
      "Epoch:  360 | Traininng loss is 0.1447\n",
      "Epoch:  361 | Traininng loss is 0.1435\n",
      "Epoch:  362 | Traininng loss is 0.1450\n",
      "Epoch:  363 | Traininng loss is 0.1444\n",
      "Epoch:  364 | Traininng loss is 0.1433\n",
      "Epoch:  365 | Traininng loss is 0.1433\n",
      "Epoch:  366 | Traininng loss is 0.1422\n",
      "Epoch:  367 | Traininng loss is 0.1443\n",
      "Epoch:  368 | Traininng loss is 0.1455\n",
      "Epoch:  369 | Traininng loss is 0.1419\n",
      "Epoch:  370 | Traininng loss is 0.1422\n",
      "Epoch:  371 | Traininng loss is 0.1428\n",
      "Epoch:  372 | Traininng loss is 0.1430\n",
      "Epoch:  373 | Traininng loss is 0.1444\n",
      "Epoch:  374 | Traininng loss is 0.1421\n",
      "Epoch:  375 | Traininng loss is 0.1395\n",
      "Epoch:  376 | Traininng loss is 0.1409\n",
      "Epoch:  377 | Traininng loss is 0.1425\n",
      "Epoch:  378 | Traininng loss is 0.1420\n",
      "Epoch:  379 | Traininng loss is 0.1435\n",
      "Epoch:  380 | Traininng loss is 0.1448\n",
      "Epoch:  381 | Traininng loss is 0.1432\n",
      "Epoch:  382 | Traininng loss is 0.1422\n",
      "Epoch:  383 | Traininng loss is 0.1420\n",
      "Epoch:  384 | Traininng loss is 0.1436\n",
      "Epoch:  385 | Traininng loss is 0.1421\n",
      "Epoch:  386 | Traininng loss is 0.1408\n",
      "Epoch:  387 | Traininng loss is 0.1417\n",
      "Epoch:  388 | Traininng loss is 0.1431\n",
      "Epoch:  389 | Traininng loss is 0.1402\n",
      "Epoch:  390 | Traininng loss is 0.1401\n",
      "Epoch:  391 | Traininng loss is 0.1413\n",
      "Epoch:  392 | Traininng loss is 0.1433\n",
      "Epoch:  393 | Traininng loss is 0.1401\n",
      "Epoch:  394 | Traininng loss is 0.1426\n",
      "Epoch:  395 | Traininng loss is 0.1413\n",
      "Epoch:  396 | Traininng loss is 0.1407\n",
      "Epoch:  397 | Traininng loss is 0.1417\n",
      "Epoch:  398 | Traininng loss is 0.1418\n",
      "Epoch:  399 | Traininng loss is 0.1409\n",
      "Epoch:  400 | Traininng loss is 0.1416\n",
      "Epoch:  401 | Traininng loss is 0.1413\n",
      "Epoch:  402 | Traininng loss is 0.1392\n",
      "Epoch:  403 | Traininng loss is 0.1425\n",
      "Epoch:  404 | Traininng loss is 0.1410\n",
      "Epoch:  405 | Traininng loss is 0.1403\n",
      "Epoch:  406 | Traininng loss is 0.1391\n",
      "Epoch:  407 | Traininng loss is 0.1401\n",
      "Epoch:  408 | Traininng loss is 0.1391\n",
      "Epoch:  409 | Traininng loss is 0.1401\n",
      "Epoch:  410 | Traininng loss is 0.1412\n",
      "Epoch:  411 | Traininng loss is 0.1389\n",
      "Epoch:  412 | Traininng loss is 0.1406\n",
      "Epoch:  413 | Traininng loss is 0.1384\n",
      "Epoch:  414 | Traininng loss is 0.1395\n",
      "Epoch:  415 | Traininng loss is 0.1366\n",
      "Epoch:  416 | Traininng loss is 0.1405\n",
      "Epoch:  417 | Traininng loss is 0.1410\n",
      "Epoch:  418 | Traininng loss is 0.1394\n",
      "Epoch:  419 | Traininng loss is 0.1408\n",
      "Epoch:  420 | Traininng loss is 0.1408\n",
      "Epoch:  421 | Traininng loss is 0.1379\n",
      "Epoch:  422 | Traininng loss is 0.1386\n",
      "Epoch:  423 | Traininng loss is 0.1398\n",
      "Epoch:  424 | Traininng loss is 0.1381\n",
      "Epoch:  425 | Traininng loss is 0.1392\n",
      "Epoch:  426 | Traininng loss is 0.1411\n",
      "Epoch:  427 | Traininng loss is 0.1377\n",
      "Epoch:  428 | Traininng loss is 0.1392\n",
      "Epoch:  429 | Traininng loss is 0.1399\n",
      "Epoch:  430 | Traininng loss is 0.1378\n",
      "Epoch:  431 | Traininng loss is 0.1384\n",
      "Epoch:  432 | Traininng loss is 0.1395\n",
      "Epoch:  433 | Traininng loss is 0.1386\n",
      "Epoch:  434 | Traininng loss is 0.1371\n",
      "Epoch:  435 | Traininng loss is 0.1383\n",
      "Epoch:  436 | Traininng loss is 0.1376\n",
      "Epoch:  437 | Traininng loss is 0.1393\n",
      "Epoch:  438 | Traininng loss is 0.1367\n",
      "Epoch:  439 | Traininng loss is 0.1376\n",
      "Epoch:  440 | Traininng loss is 0.1362\n",
      "Epoch:  441 | Traininng loss is 0.1385\n",
      "Epoch:  442 | Traininng loss is 0.1389\n",
      "Epoch:  443 | Traininng loss is 0.1364\n",
      "Epoch:  444 | Traininng loss is 0.1374\n",
      "Epoch:  445 | Traininng loss is 0.1368\n",
      "Epoch:  446 | Traininng loss is 0.1379\n",
      "Epoch:  447 | Traininng loss is 0.1371\n",
      "Epoch:  448 | Traininng loss is 0.1370\n",
      "Epoch:  449 | Traininng loss is 0.1357\n",
      "Epoch:  450 | Traininng loss is 0.1373\n",
      "Epoch:  451 | Traininng loss is 0.1365\n",
      "Epoch:  452 | Traininng loss is 0.1378\n",
      "Epoch:  453 | Traininng loss is 0.1364\n",
      "Epoch:  454 | Traininng loss is 0.1382\n",
      "Epoch:  455 | Traininng loss is 0.1383\n",
      "Epoch:  456 | Traininng loss is 0.1358\n",
      "Epoch:  457 | Traininng loss is 0.1367\n",
      "Epoch:  458 | Traininng loss is 0.1369\n",
      "Epoch:  459 | Traininng loss is 0.1341\n",
      "Epoch:  460 | Traininng loss is 0.1371\n",
      "Epoch:  461 | Traininng loss is 0.1371\n",
      "Epoch:  462 | Traininng loss is 0.1357\n",
      "Epoch:  463 | Traininng loss is 0.1368\n",
      "Epoch:  464 | Traininng loss is 0.1367\n",
      "Epoch:  465 | Traininng loss is 0.1359\n",
      "Epoch:  466 | Traininng loss is 0.1351\n",
      "Epoch:  467 | Traininng loss is 0.1387\n",
      "Epoch:  468 | Traininng loss is 0.1357\n",
      "Epoch:  469 | Traininng loss is 0.1356\n",
      "Epoch:  470 | Traininng loss is 0.1372\n",
      "Epoch:  471 | Traininng loss is 0.1343\n",
      "Epoch:  472 | Traininng loss is 0.1369\n",
      "Epoch:  473 | Traininng loss is 0.1338\n",
      "Epoch:  474 | Traininng loss is 0.1393\n",
      "Epoch:  475 | Traininng loss is 0.1354\n",
      "Epoch:  476 | Traininng loss is 0.1361\n",
      "Epoch:  477 | Traininng loss is 0.1354\n",
      "Epoch:  478 | Traininng loss is 0.1364\n",
      "Epoch:  479 | Traininng loss is 0.1350\n",
      "Epoch:  480 | Traininng loss is 0.1352\n",
      "Epoch:  481 | Traininng loss is 0.1359\n",
      "Epoch:  482 | Traininng loss is 0.1357\n",
      "Epoch:  483 | Traininng loss is 0.1363\n",
      "Epoch:  484 | Traininng loss is 0.1355\n",
      "Epoch:  485 | Traininng loss is 0.1350\n",
      "Epoch:  486 | Traininng loss is 0.1331\n",
      "Epoch:  487 | Traininng loss is 0.1344\n",
      "Epoch:  488 | Traininng loss is 0.1335\n",
      "Epoch:  489 | Traininng loss is 0.1355\n",
      "Epoch:  490 | Traininng loss is 0.1356\n",
      "Epoch:  491 | Traininng loss is 0.1344\n",
      "Epoch:  492 | Traininng loss is 0.1367\n",
      "Epoch:  493 | Traininng loss is 0.1337\n",
      "Epoch:  494 | Traininng loss is 0.1347\n",
      "Epoch:  495 | Traininng loss is 0.1360\n",
      "Epoch:  496 | Traininng loss is 0.1338\n",
      "Epoch:  497 | Traininng loss is 0.1341\n",
      "Epoch:  498 | Traininng loss is 0.1325\n",
      "Epoch:  499 | Traininng loss is 0.1342\n",
      "Epoch:  500 | Traininng loss is 0.1343\n",
      "Epoch:  501 | Traininng loss is 0.1342\n",
      "Epoch:  502 | Traininng loss is 0.1328\n",
      "Epoch:  503 | Traininng loss is 0.1335\n",
      "Epoch:  504 | Traininng loss is 0.1337\n",
      "Epoch:  505 | Traininng loss is 0.1334\n",
      "Epoch:  506 | Traininng loss is 0.1343\n",
      "Epoch:  507 | Traininng loss is 0.1345\n",
      "Epoch:  508 | Traininng loss is 0.1332\n",
      "Epoch:  509 | Traininng loss is 0.1310\n",
      "Epoch:  510 | Traininng loss is 0.1335\n",
      "Epoch:  511 | Traininng loss is 0.1334\n",
      "Epoch:  512 | Traininng loss is 0.1309\n",
      "Epoch:  513 | Traininng loss is 0.1335\n",
      "Epoch:  514 | Traininng loss is 0.1349\n",
      "Epoch:  515 | Traininng loss is 0.1335\n",
      "Epoch:  516 | Traininng loss is 0.1321\n",
      "Epoch:  517 | Traininng loss is 0.1330\n",
      "Epoch:  518 | Traininng loss is 0.1319\n",
      "Epoch:  519 | Traininng loss is 0.1321\n",
      "Epoch:  520 | Traininng loss is 0.1320\n",
      "Epoch:  521 | Traininng loss is 0.1342\n",
      "Epoch:  522 | Traininng loss is 0.1334\n",
      "Epoch:  523 | Traininng loss is 0.1316\n",
      "Epoch:  524 | Traininng loss is 0.1334\n",
      "Epoch:  525 | Traininng loss is 0.1353\n",
      "Epoch:  526 | Traininng loss is 0.1341\n",
      "Epoch:  527 | Traininng loss is 0.1329\n",
      "Epoch:  528 | Traininng loss is 0.1325\n",
      "Epoch:  529 | Traininng loss is 0.1336\n",
      "Epoch:  530 | Traininng loss is 0.1317\n",
      "Epoch:  531 | Traininng loss is 0.1345\n",
      "Epoch:  532 | Traininng loss is 0.1331\n",
      "Epoch:  533 | Traininng loss is 0.1336\n",
      "Epoch:  534 | Traininng loss is 0.1296\n",
      "Epoch:  535 | Traininng loss is 0.1341\n",
      "Epoch:  536 | Traininng loss is 0.1338\n",
      "Epoch:  537 | Traininng loss is 0.1299\n",
      "Epoch:  538 | Traininng loss is 0.1324\n",
      "Epoch:  539 | Traininng loss is 0.1315\n",
      "Epoch:  540 | Traininng loss is 0.1343\n",
      "Epoch:  541 | Traininng loss is 0.1322\n",
      "Epoch:  542 | Traininng loss is 0.1314\n",
      "Epoch:  543 | Traininng loss is 0.1340\n",
      "Epoch:  544 | Traininng loss is 0.1335\n",
      "Epoch:  545 | Traininng loss is 0.1314\n",
      "Epoch:  546 | Traininng loss is 0.1321\n",
      "Epoch:  547 | Traininng loss is 0.1301\n",
      "Epoch:  548 | Traininng loss is 0.1337\n",
      "Epoch:  549 | Traininng loss is 0.1342\n",
      "Epoch:  550 | Traininng loss is 0.1316\n",
      "Epoch:  551 | Traininng loss is 0.1319\n",
      "Epoch:  552 | Traininng loss is 0.1329\n",
      "Epoch:  553 | Traininng loss is 0.1301\n",
      "Epoch:  554 | Traininng loss is 0.1321\n",
      "Epoch:  555 | Traininng loss is 0.1313\n",
      "Epoch:  556 | Traininng loss is 0.1313\n",
      "Epoch:  557 | Traininng loss is 0.1337\n",
      "Epoch:  558 | Traininng loss is 0.1304\n",
      "Epoch:  559 | Traininng loss is 0.1307\n",
      "Epoch:  560 | Traininng loss is 0.1326\n",
      "Epoch:  561 | Traininng loss is 0.1327\n",
      "Epoch:  562 | Traininng loss is 0.1301\n",
      "Epoch:  563 | Traininng loss is 0.1317\n",
      "Epoch:  564 | Traininng loss is 0.1314\n",
      "Epoch:  565 | Traininng loss is 0.1297\n",
      "Epoch:  566 | Traininng loss is 0.1315\n",
      "Epoch:  567 | Traininng loss is 0.1283\n",
      "Epoch:  568 | Traininng loss is 0.1326\n",
      "Epoch:  569 | Traininng loss is 0.1319\n",
      "Epoch:  570 | Traininng loss is 0.1297\n",
      "Epoch:  571 | Traininng loss is 0.1303\n",
      "Epoch:  572 | Traininng loss is 0.1294\n",
      "Epoch:  573 | Traininng loss is 0.1316\n",
      "Epoch:  574 | Traininng loss is 0.1316\n",
      "Epoch:  575 | Traininng loss is 0.1313\n",
      "Epoch:  576 | Traininng loss is 0.1318\n",
      "Epoch:  577 | Traininng loss is 0.1301\n",
      "Epoch:  578 | Traininng loss is 0.1326\n",
      "Epoch:  579 | Traininng loss is 0.1295\n",
      "Epoch:  580 | Traininng loss is 0.1296\n",
      "Epoch:  581 | Traininng loss is 0.1301\n",
      "Epoch:  582 | Traininng loss is 0.1298\n",
      "Epoch:  583 | Traininng loss is 0.1293\n",
      "Epoch:  584 | Traininng loss is 0.1318\n",
      "Epoch:  585 | Traininng loss is 0.1310\n",
      "Epoch:  586 | Traininng loss is 0.1306\n",
      "Epoch:  587 | Traininng loss is 0.1298\n",
      "Epoch:  588 | Traininng loss is 0.1292\n",
      "Epoch:  589 | Traininng loss is 0.1304\n",
      "Epoch:  590 | Traininng loss is 0.1310\n",
      "Epoch:  591 | Traininng loss is 0.1307\n",
      "Epoch:  592 | Traininng loss is 0.1306\n",
      "Epoch:  593 | Traininng loss is 0.1310\n",
      "Epoch:  594 | Traininng loss is 0.1300\n",
      "Epoch:  595 | Traininng loss is 0.1294\n",
      "Epoch:  596 | Traininng loss is 0.1295\n",
      "Epoch:  597 | Traininng loss is 0.1299\n",
      "Epoch:  598 | Traininng loss is 0.1302\n",
      "Epoch:  599 | Traininng loss is 0.1316\n",
      "Epoch:  600 | Traininng loss is 0.1298\n",
      "Epoch:  601 | Traininng loss is 0.1289\n",
      "Epoch:  602 | Traininng loss is 0.1284\n",
      "Epoch:  603 | Traininng loss is 0.1298\n",
      "Epoch:  604 | Traininng loss is 0.1306\n",
      "Epoch:  605 | Traininng loss is 0.1286\n",
      "Epoch:  606 | Traininng loss is 0.1277\n",
      "Epoch:  607 | Traininng loss is 0.1308\n",
      "Epoch:  608 | Traininng loss is 0.1298\n",
      "Epoch:  609 | Traininng loss is 0.1282\n",
      "Epoch:  610 | Traininng loss is 0.1305\n",
      "Epoch:  611 | Traininng loss is 0.1297\n",
      "Epoch:  612 | Traininng loss is 0.1288\n",
      "Epoch:  613 | Traininng loss is 0.1286\n",
      "Epoch:  614 | Traininng loss is 0.1291\n",
      "Epoch:  615 | Traininng loss is 0.1287\n",
      "Epoch:  616 | Traininng loss is 0.1305\n",
      "Epoch:  617 | Traininng loss is 0.1295\n",
      "Epoch:  618 | Traininng loss is 0.1282\n",
      "Epoch:  619 | Traininng loss is 0.1298\n",
      "Epoch:  620 | Traininng loss is 0.1302\n",
      "Epoch:  621 | Traininng loss is 0.1299\n",
      "Epoch:  622 | Traininng loss is 0.1295\n",
      "Epoch:  623 | Traininng loss is 0.1296\n",
      "Epoch:  624 | Traininng loss is 0.1283\n",
      "Epoch:  625 | Traininng loss is 0.1274\n",
      "Epoch:  626 | Traininng loss is 0.1297\n",
      "Epoch:  627 | Traininng loss is 0.1320\n",
      "Epoch:  628 | Traininng loss is 0.1284\n",
      "Epoch:  629 | Traininng loss is 0.1287\n",
      "Epoch:  630 | Traininng loss is 0.1289\n",
      "Epoch:  631 | Traininng loss is 0.1275\n",
      "Epoch:  632 | Traininng loss is 0.1302\n",
      "Epoch:  633 | Traininng loss is 0.1292\n",
      "Epoch:  634 | Traininng loss is 0.1306\n",
      "Epoch:  635 | Traininng loss is 0.1269\n",
      "Epoch:  636 | Traininng loss is 0.1300\n",
      "Epoch:  637 | Traininng loss is 0.1270\n",
      "Epoch:  638 | Traininng loss is 0.1298\n",
      "Epoch:  639 | Traininng loss is 0.1287\n",
      "Epoch:  640 | Traininng loss is 0.1301\n",
      "Epoch:  641 | Traininng loss is 0.1294\n",
      "Epoch:  642 | Traininng loss is 0.1288\n",
      "Epoch:  643 | Traininng loss is 0.1291\n",
      "Epoch:  644 | Traininng loss is 0.1298\n",
      "Epoch:  645 | Traininng loss is 0.1265\n",
      "Epoch:  646 | Traininng loss is 0.1281\n",
      "Epoch:  647 | Traininng loss is 0.1277\n",
      "Epoch:  648 | Traininng loss is 0.1281\n",
      "Epoch:  649 | Traininng loss is 0.1292\n",
      "Epoch:  650 | Traininng loss is 0.1280\n",
      "Epoch:  651 | Traininng loss is 0.1288\n",
      "Epoch:  652 | Traininng loss is 0.1278\n",
      "Epoch:  653 | Traininng loss is 0.1288\n",
      "Epoch:  654 | Traininng loss is 0.1249\n",
      "Epoch:  655 | Traininng loss is 0.1268\n",
      "Epoch:  656 | Traininng loss is 0.1281\n",
      "Epoch:  657 | Traininng loss is 0.1273\n",
      "Epoch:  658 | Traininng loss is 0.1272\n",
      "Epoch:  659 | Traininng loss is 0.1271\n",
      "Epoch:  660 | Traininng loss is 0.1272\n",
      "Epoch:  661 | Traininng loss is 0.1276\n",
      "Epoch:  662 | Traininng loss is 0.1281\n",
      "Epoch:  663 | Traininng loss is 0.1281\n",
      "Epoch:  664 | Traininng loss is 0.1267\n",
      "Epoch:  665 | Traininng loss is 0.1271\n",
      "Epoch:  666 | Traininng loss is 0.1272\n",
      "Epoch:  667 | Traininng loss is 0.1287\n",
      "Epoch:  668 | Traininng loss is 0.1249\n",
      "Epoch:  669 | Traininng loss is 0.1284\n",
      "Epoch:  670 | Traininng loss is 0.1270\n",
      "Epoch:  671 | Traininng loss is 0.1269\n",
      "Epoch:  672 | Traininng loss is 0.1274\n",
      "Epoch:  673 | Traininng loss is 0.1274\n",
      "Epoch:  674 | Traininng loss is 0.1285\n",
      "Epoch:  675 | Traininng loss is 0.1269\n",
      "Epoch:  676 | Traininng loss is 0.1282\n",
      "Epoch:  677 | Traininng loss is 0.1281\n",
      "Epoch:  678 | Traininng loss is 0.1264\n",
      "Epoch:  679 | Traininng loss is 0.1266\n",
      "Epoch:  680 | Traininng loss is 0.1273\n",
      "Epoch:  681 | Traininng loss is 0.1264\n",
      "Epoch:  682 | Traininng loss is 0.1285\n",
      "Epoch:  683 | Traininng loss is 0.1268\n",
      "Epoch:  684 | Traininng loss is 0.1270\n",
      "Epoch:  685 | Traininng loss is 0.1262\n",
      "Epoch:  686 | Traininng loss is 0.1262\n",
      "Epoch:  687 | Traininng loss is 0.1269\n",
      "Epoch:  688 | Traininng loss is 0.1251\n",
      "Epoch:  689 | Traininng loss is 0.1270\n",
      "Epoch:  690 | Traininng loss is 0.1263\n",
      "Epoch:  691 | Traininng loss is 0.1276\n",
      "Epoch:  692 | Traininng loss is 0.1264\n",
      "Epoch:  693 | Traininng loss is 0.1256\n",
      "Epoch:  694 | Traininng loss is 0.1265\n",
      "Epoch:  695 | Traininng loss is 0.1268\n",
      "Epoch:  696 | Traininng loss is 0.1255\n",
      "Epoch:  697 | Traininng loss is 0.1267\n",
      "Epoch:  698 | Traininng loss is 0.1241\n",
      "Epoch:  699 | Traininng loss is 0.1266\n",
      "Epoch:  700 | Traininng loss is 0.1266\n",
      "Epoch:  701 | Traininng loss is 0.1251\n",
      "Epoch:  702 | Traininng loss is 0.1259\n",
      "Epoch:  703 | Traininng loss is 0.1259\n",
      "Epoch:  704 | Traininng loss is 0.1249\n",
      "Epoch:  705 | Traininng loss is 0.1266\n",
      "Epoch:  706 | Traininng loss is 0.1241\n",
      "Epoch:  707 | Traininng loss is 0.1263\n",
      "Epoch:  708 | Traininng loss is 0.1247\n",
      "Epoch:  709 | Traininng loss is 0.1268\n",
      "Epoch:  710 | Traininng loss is 0.1271\n",
      "Epoch:  711 | Traininng loss is 0.1275\n",
      "Epoch:  712 | Traininng loss is 0.1264\n",
      "Epoch:  713 | Traininng loss is 0.1273\n",
      "Epoch:  714 | Traininng loss is 0.1258\n",
      "Epoch:  715 | Traininng loss is 0.1252\n",
      "Epoch:  716 | Traininng loss is 0.1244\n",
      "Epoch:  717 | Traininng loss is 0.1259\n",
      "Epoch:  718 | Traininng loss is 0.1264\n",
      "Epoch:  719 | Traininng loss is 0.1258\n",
      "Epoch:  720 | Traininng loss is 0.1247\n",
      "Epoch:  721 | Traininng loss is 0.1278\n",
      "Epoch:  722 | Traininng loss is 0.1239\n",
      "Epoch:  723 | Traininng loss is 0.1250\n",
      "Epoch:  724 | Traininng loss is 0.1257\n",
      "Epoch:  725 | Traininng loss is 0.1241\n",
      "Epoch:  726 | Traininng loss is 0.1261\n",
      "Epoch:  727 | Traininng loss is 0.1243\n",
      "Epoch:  728 | Traininng loss is 0.1270\n",
      "Epoch:  729 | Traininng loss is 0.1263\n",
      "Epoch:  730 | Traininng loss is 0.1251\n",
      "Epoch:  731 | Traininng loss is 0.1262\n",
      "Epoch:  732 | Traininng loss is 0.1253\n",
      "Epoch:  733 | Traininng loss is 0.1252\n",
      "Epoch:  734 | Traininng loss is 0.1258\n",
      "Epoch:  735 | Traininng loss is 0.1237\n",
      "Epoch:  736 | Traininng loss is 0.1241\n",
      "Epoch:  737 | Traininng loss is 0.1259\n",
      "Epoch:  738 | Traininng loss is 0.1249\n",
      "Epoch:  739 | Traininng loss is 0.1257\n",
      "Epoch:  740 | Traininng loss is 0.1247\n",
      "Epoch:  741 | Traininng loss is 0.1271\n",
      "Epoch:  742 | Traininng loss is 0.1246\n",
      "Epoch:  743 | Traininng loss is 0.1252\n",
      "Epoch:  744 | Traininng loss is 0.1242\n",
      "Epoch:  745 | Traininng loss is 0.1262\n",
      "Epoch:  746 | Traininng loss is 0.1252\n",
      "Epoch:  747 | Traininng loss is 0.1258\n",
      "Epoch:  748 | Traininng loss is 0.1266\n",
      "Epoch:  749 | Traininng loss is 0.1243\n",
      "Epoch:  750 | Traininng loss is 0.1254\n",
      "Epoch:  751 | Traininng loss is 0.1241\n",
      "Epoch:  752 | Traininng loss is 0.1259\n",
      "Epoch:  753 | Traininng loss is 0.1244\n",
      "Epoch:  754 | Traininng loss is 0.1237\n",
      "Epoch:  755 | Traininng loss is 0.1242\n",
      "Epoch:  756 | Traininng loss is 0.1240\n",
      "Epoch:  757 | Traininng loss is 0.1253\n",
      "Epoch:  758 | Traininng loss is 0.1249\n",
      "Epoch:  759 | Traininng loss is 0.1238\n",
      "Epoch:  760 | Traininng loss is 0.1260\n",
      "Epoch:  761 | Traininng loss is 0.1243\n",
      "Epoch:  762 | Traininng loss is 0.1261\n",
      "Epoch:  763 | Traininng loss is 0.1221\n",
      "Epoch:  764 | Traininng loss is 0.1225\n",
      "Epoch:  765 | Traininng loss is 0.1240\n",
      "Epoch:  766 | Traininng loss is 0.1234\n",
      "Epoch:  767 | Traininng loss is 0.1236\n",
      "Epoch:  768 | Traininng loss is 0.1244\n",
      "Epoch:  769 | Traininng loss is 0.1249\n",
      "Epoch:  770 | Traininng loss is 0.1250\n",
      "Epoch:  771 | Traininng loss is 0.1246\n",
      "Epoch:  772 | Traininng loss is 0.1246\n",
      "Epoch:  773 | Traininng loss is 0.1248\n",
      "Epoch:  774 | Traininng loss is 0.1261\n",
      "Epoch:  775 | Traininng loss is 0.1237\n",
      "Epoch:  776 | Traininng loss is 0.1223\n",
      "Epoch:  777 | Traininng loss is 0.1240\n",
      "Epoch:  778 | Traininng loss is 0.1247\n",
      "Epoch:  779 | Traininng loss is 0.1243\n",
      "Epoch:  780 | Traininng loss is 0.1237\n",
      "Epoch:  781 | Traininng loss is 0.1237\n",
      "Epoch:  782 | Traininng loss is 0.1243\n",
      "Epoch:  783 | Traininng loss is 0.1247\n",
      "Epoch:  784 | Traininng loss is 0.1258\n",
      "Epoch:  785 | Traininng loss is 0.1243\n",
      "Epoch:  786 | Traininng loss is 0.1237\n",
      "Epoch:  787 | Traininng loss is 0.1234\n",
      "Epoch:  788 | Traininng loss is 0.1248\n",
      "Epoch:  789 | Traininng loss is 0.1256\n",
      "Epoch:  790 | Traininng loss is 0.1244\n",
      "Epoch:  791 | Traininng loss is 0.1239\n",
      "Epoch:  792 | Traininng loss is 0.1238\n",
      "Epoch:  793 | Traininng loss is 0.1230\n",
      "Epoch:  794 | Traininng loss is 0.1241\n",
      "Epoch:  795 | Traininng loss is 0.1253\n",
      "Epoch:  796 | Traininng loss is 0.1227\n",
      "Epoch:  797 | Traininng loss is 0.1233\n",
      "Epoch:  798 | Traininng loss is 0.1215\n",
      "Epoch:  799 | Traininng loss is 0.1236\n",
      "Epoch:  800 | Traininng loss is 0.1220\n",
      "Epoch:  801 | Traininng loss is 0.1246\n",
      "Epoch:  802 | Traininng loss is 0.1223\n",
      "Epoch:  803 | Traininng loss is 0.1246\n",
      "Epoch:  804 | Traininng loss is 0.1236\n",
      "Epoch:  805 | Traininng loss is 0.1227\n",
      "Epoch:  806 | Traininng loss is 0.1235\n",
      "Epoch:  807 | Traininng loss is 0.1229\n",
      "Epoch:  808 | Traininng loss is 0.1221\n",
      "Epoch:  809 | Traininng loss is 0.1232\n",
      "Epoch:  810 | Traininng loss is 0.1236\n",
      "Epoch:  811 | Traininng loss is 0.1224\n",
      "Epoch:  812 | Traininng loss is 0.1236\n",
      "Epoch:  813 | Traininng loss is 0.1226\n",
      "Epoch:  814 | Traininng loss is 0.1236\n",
      "Epoch:  815 | Traininng loss is 0.1240\n",
      "Epoch:  816 | Traininng loss is 0.1244\n",
      "Epoch:  817 | Traininng loss is 0.1230\n",
      "Epoch:  818 | Traininng loss is 0.1232\n",
      "Epoch:  819 | Traininng loss is 0.1236\n",
      "Epoch:  820 | Traininng loss is 0.1226\n",
      "Epoch:  821 | Traininng loss is 0.1255\n",
      "Epoch:  822 | Traininng loss is 0.1226\n",
      "Epoch:  823 | Traininng loss is 0.1228\n",
      "Epoch:  824 | Traininng loss is 0.1225\n",
      "Epoch:  825 | Traininng loss is 0.1230\n",
      "Epoch:  826 | Traininng loss is 0.1225\n",
      "Epoch:  827 | Traininng loss is 0.1239\n",
      "Epoch:  828 | Traininng loss is 0.1216\n",
      "Epoch:  829 | Traininng loss is 0.1232\n",
      "Epoch:  830 | Traininng loss is 0.1214\n",
      "Epoch:  831 | Traininng loss is 0.1236\n",
      "Epoch:  832 | Traininng loss is 0.1227\n",
      "Epoch:  833 | Traininng loss is 0.1236\n",
      "Epoch:  834 | Traininng loss is 0.1228\n",
      "Epoch:  835 | Traininng loss is 0.1226\n",
      "Epoch:  836 | Traininng loss is 0.1227\n",
      "Epoch:  837 | Traininng loss is 0.1224\n",
      "Epoch:  838 | Traininng loss is 0.1219\n",
      "Epoch:  839 | Traininng loss is 0.1229\n",
      "Epoch:  840 | Traininng loss is 0.1217\n",
      "Epoch:  841 | Traininng loss is 0.1223\n",
      "Epoch:  842 | Traininng loss is 0.1231\n",
      "Epoch:  843 | Traininng loss is 0.1216\n",
      "Epoch:  844 | Traininng loss is 0.1225\n",
      "Epoch:  845 | Traininng loss is 0.1219\n",
      "Epoch:  846 | Traininng loss is 0.1232\n",
      "Epoch:  847 | Traininng loss is 0.1233\n",
      "Epoch:  848 | Traininng loss is 0.1218\n",
      "Epoch:  849 | Traininng loss is 0.1211\n",
      "Epoch:  850 | Traininng loss is 0.1219\n",
      "Epoch:  851 | Traininng loss is 0.1211\n",
      "Epoch:  852 | Traininng loss is 0.1224\n",
      "Epoch:  853 | Traininng loss is 0.1208\n",
      "Epoch:  854 | Traininng loss is 0.1206\n",
      "Epoch:  855 | Traininng loss is 0.1202\n",
      "Epoch:  856 | Traininng loss is 0.1219\n",
      "Epoch:  857 | Traininng loss is 0.1221\n",
      "Epoch:  858 | Traininng loss is 0.1222\n",
      "Epoch:  859 | Traininng loss is 0.1224\n",
      "Epoch:  860 | Traininng loss is 0.1224\n",
      "Epoch:  861 | Traininng loss is 0.1212\n",
      "Epoch:  862 | Traininng loss is 0.1226\n",
      "Epoch:  863 | Traininng loss is 0.1220\n",
      "Epoch:  864 | Traininng loss is 0.1215\n",
      "Epoch:  865 | Traininng loss is 0.1214\n",
      "Epoch:  866 | Traininng loss is 0.1219\n",
      "Epoch:  867 | Traininng loss is 0.1212\n",
      "Epoch:  868 | Traininng loss is 0.1209\n",
      "Epoch:  869 | Traininng loss is 0.1199\n",
      "Epoch:  870 | Traininng loss is 0.1211\n",
      "Epoch:  871 | Traininng loss is 0.1209\n",
      "Epoch:  872 | Traininng loss is 0.1206\n",
      "Epoch:  873 | Traininng loss is 0.1219\n",
      "Epoch:  874 | Traininng loss is 0.1203\n",
      "Epoch:  875 | Traininng loss is 0.1216\n",
      "Epoch:  876 | Traininng loss is 0.1219\n",
      "Epoch:  877 | Traininng loss is 0.1226\n",
      "Epoch:  878 | Traininng loss is 0.1207\n",
      "Epoch:  879 | Traininng loss is 0.1223\n",
      "Epoch:  880 | Traininng loss is 0.1219\n",
      "Epoch:  881 | Traininng loss is 0.1203\n",
      "Epoch:  882 | Traininng loss is 0.1209\n",
      "Epoch:  883 | Traininng loss is 0.1213\n",
      "Epoch:  884 | Traininng loss is 0.1219\n",
      "Epoch:  885 | Traininng loss is 0.1220\n",
      "Epoch:  886 | Traininng loss is 0.1234\n",
      "Epoch:  887 | Traininng loss is 0.1222\n",
      "Epoch:  888 | Traininng loss is 0.1218\n",
      "Epoch:  889 | Traininng loss is 0.1211\n",
      "Epoch:  890 | Traininng loss is 0.1211\n",
      "Epoch:  891 | Traininng loss is 0.1205\n",
      "Epoch:  892 | Traininng loss is 0.1205\n",
      "Epoch:  893 | Traininng loss is 0.1206\n",
      "Epoch:  894 | Traininng loss is 0.1215\n",
      "Epoch:  895 | Traininng loss is 0.1213\n",
      "Epoch:  896 | Traininng loss is 0.1202\n",
      "Epoch:  897 | Traininng loss is 0.1203\n",
      "Epoch:  898 | Traininng loss is 0.1206\n",
      "Epoch:  899 | Traininng loss is 0.1206\n",
      "Epoch:  900 | Traininng loss is 0.1207\n",
      "Epoch:  901 | Traininng loss is 0.1211\n",
      "Epoch:  902 | Traininng loss is 0.1205\n",
      "Epoch:  903 | Traininng loss is 0.1208\n",
      "Epoch:  904 | Traininng loss is 0.1197\n",
      "Epoch:  905 | Traininng loss is 0.1205\n",
      "Epoch:  906 | Traininng loss is 0.1208\n",
      "Epoch:  907 | Traininng loss is 0.1222\n",
      "Epoch:  908 | Traininng loss is 0.1195\n",
      "Epoch:  909 | Traininng loss is 0.1205\n",
      "Epoch:  910 | Traininng loss is 0.1189\n",
      "Epoch:  911 | Traininng loss is 0.1191\n",
      "Epoch:  912 | Traininng loss is 0.1207\n",
      "Epoch:  913 | Traininng loss is 0.1193\n",
      "Epoch:  914 | Traininng loss is 0.1206\n",
      "Epoch:  915 | Traininng loss is 0.1206\n",
      "Epoch:  916 | Traininng loss is 0.1198\n",
      "Epoch:  917 | Traininng loss is 0.1202\n",
      "Epoch:  918 | Traininng loss is 0.1212\n",
      "Epoch:  919 | Traininng loss is 0.1219\n",
      "Epoch:  920 | Traininng loss is 0.1206\n",
      "Epoch:  921 | Traininng loss is 0.1201\n",
      "Epoch:  922 | Traininng loss is 0.1197\n",
      "Epoch:  923 | Traininng loss is 0.1205\n",
      "Epoch:  924 | Traininng loss is 0.1189\n",
      "Epoch:  925 | Traininng loss is 0.1200\n",
      "Epoch:  926 | Traininng loss is 0.1191\n",
      "Epoch:  927 | Traininng loss is 0.1214\n",
      "Epoch:  928 | Traininng loss is 0.1199\n",
      "Epoch:  929 | Traininng loss is 0.1211\n",
      "Epoch:  930 | Traininng loss is 0.1209\n",
      "Epoch:  931 | Traininng loss is 0.1203\n",
      "Epoch:  932 | Traininng loss is 0.1205\n",
      "Epoch:  933 | Traininng loss is 0.1214\n",
      "Epoch:  934 | Traininng loss is 0.1212\n",
      "Epoch:  935 | Traininng loss is 0.1203\n",
      "Epoch:  936 | Traininng loss is 0.1199\n",
      "Epoch:  937 | Traininng loss is 0.1203\n",
      "Epoch:  938 | Traininng loss is 0.1207\n",
      "Epoch:  939 | Traininng loss is 0.1205\n",
      "Epoch:  940 | Traininng loss is 0.1195\n",
      "Epoch:  941 | Traininng loss is 0.1198\n",
      "Epoch:  942 | Traininng loss is 0.1208\n",
      "Epoch:  943 | Traininng loss is 0.1201\n",
      "Epoch:  944 | Traininng loss is 0.1204\n",
      "Epoch:  945 | Traininng loss is 0.1194\n",
      "Epoch:  946 | Traininng loss is 0.1201\n",
      "Epoch:  947 | Traininng loss is 0.1198\n",
      "Epoch:  948 | Traininng loss is 0.1185\n",
      "Epoch:  949 | Traininng loss is 0.1208\n",
      "Epoch:  950 | Traininng loss is 0.1198\n",
      "Epoch:  951 | Traininng loss is 0.1192\n",
      "Epoch:  952 | Traininng loss is 0.1212\n",
      "Epoch:  953 | Traininng loss is 0.1195\n",
      "Epoch:  954 | Traininng loss is 0.1197\n",
      "Epoch:  955 | Traininng loss is 0.1197\n",
      "Epoch:  956 | Traininng loss is 0.1201\n",
      "Epoch:  957 | Traininng loss is 0.1185\n",
      "Epoch:  958 | Traininng loss is 0.1193\n",
      "Epoch:  959 | Traininng loss is 0.1192\n",
      "Epoch:  960 | Traininng loss is 0.1192\n",
      "Epoch:  961 | Traininng loss is 0.1191\n",
      "Epoch:  962 | Traininng loss is 0.1206\n",
      "Epoch:  963 | Traininng loss is 0.1194\n",
      "Epoch:  964 | Traininng loss is 0.1200\n",
      "Epoch:  965 | Traininng loss is 0.1203\n",
      "Epoch:  966 | Traininng loss is 0.1193\n",
      "Epoch:  967 | Traininng loss is 0.1206\n",
      "Epoch:  968 | Traininng loss is 0.1179\n",
      "Epoch:  969 | Traininng loss is 0.1192\n",
      "Epoch:  970 | Traininng loss is 0.1192\n",
      "Epoch:  971 | Traininng loss is 0.1199\n",
      "Epoch:  972 | Traininng loss is 0.1205\n",
      "Epoch:  973 | Traininng loss is 0.1189\n",
      "Epoch:  974 | Traininng loss is 0.1185\n",
      "Epoch:  975 | Traininng loss is 0.1177\n",
      "Epoch:  976 | Traininng loss is 0.1193\n",
      "Epoch:  977 | Traininng loss is 0.1181\n",
      "Epoch:  978 | Traininng loss is 0.1197\n",
      "Epoch:  979 | Traininng loss is 0.1190\n",
      "Epoch:  980 | Traininng loss is 0.1183\n",
      "Epoch:  981 | Traininng loss is 0.1198\n",
      "Epoch:  982 | Traininng loss is 0.1183\n",
      "Epoch:  983 | Traininng loss is 0.1191\n",
      "Epoch:  984 | Traininng loss is 0.1199\n",
      "Epoch:  985 | Traininng loss is 0.1190\n",
      "Epoch:  986 | Traininng loss is 0.1183\n",
      "Epoch:  987 | Traininng loss is 0.1183\n",
      "Epoch:  988 | Traininng loss is 0.1188\n",
      "Epoch:  989 | Traininng loss is 0.1187\n",
      "Epoch:  990 | Traininng loss is 0.1208\n",
      "Epoch:  991 | Traininng loss is 0.1191\n",
      "Epoch:  992 | Traininng loss is 0.1195\n",
      "Epoch:  993 | Traininng loss is 0.1190\n",
      "Epoch:  994 | Traininng loss is 0.1193\n",
      "Epoch:  995 | Traininng loss is 0.1191\n",
      "Epoch:  996 | Traininng loss is 0.1191\n",
      "Epoch:  997 | Traininng loss is 0.1189\n",
      "Epoch:  998 | Traininng loss is 0.1179\n",
      "Epoch:  999 | Traininng loss is 0.1205\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    ed1, ed2, de1, de2 = graph2vec(TrainXTensor, TrainSTensor)\n",
    "    loss = loss_func(TrainXTensor,de1,TrainSTensor,de2,ed1,ed2)      # mean square error\n",
    "    optimizer.zero_grad()               # clear gradients for this training step\n",
    "    loss.backward()                     # backpropagation, compute gradients\n",
    "    optimizer.step()\n",
    "    print('Epoch: ',i, '|' ,'Traininng loss is %.4f' %loss)                    # apply gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed1, ed2, de1, de2=graph2vec(TrainXTensor, TrainSTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embed = ed1.data.numpy()\n",
    "df_Embed = pd.DataFrame(Embed)\n",
    "df_Embed.columns = ['C0','C1','C2','C3','C4','C5','C6','C7','C8','C9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor(n_neighbors=10, contamination=0.2, algorithm='auto', n_jobs=-1, novelty=True)\n",
    "lof.fit(Embed)\n",
    "y_pred_outliers_LOF = lof.predict(Embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1095"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score=list(np.abs(lof.negative_outlier_factor_))\n",
    "len(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1095"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kid=[]\n",
    "for i in range(len(score)):\n",
    "    kid.append(26)\n",
    "\n",
    "len(kid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datelist(beginDate, endDate):\n",
    "    # beginDate, endDate是形如‘20160601’的字符串或datetime格式\n",
    "    date_l=[datetime.strftime(x,'%Y-%m-%d') for x in list(pd.date_range(start=beginDate, end=endDate))]\n",
    "    return date_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1095"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date=datelist('20180101','20201230')\n",
    "len(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1={'kid': 1, 'date': 1, 'score': 1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'kid': kid, 'date': date, 'score': score} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
